{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "1. Prepare the data: For each document, split it into chunks of appropriate size (e.g., paragraphs or a fixed number of sentences). Pair each chunk with the question attached to the document, creating a dataset of question-chunk pairs.\n",
    "\n",
    "2. Input preparation: Tokenize the questions and chunks, and create input sequences by concatenating the tokenized question and chunk, separated by a special separator token like [SEP]. The input sequence will look like this: [CLS] question_tokens [SEP] chunk_tokens [SEP]. Also, create attention masks to indicate the positions of tokens in the input sequences.\n",
    "\n",
    "3. Model configuration: Select a pre-trained BERT model and configure it for a masked language modeling (MLM) task. In this task, the model learns to predict masked tokens in the input sequence based on the context provided by the remaining tokens.\n",
    "\n",
    "4. Data masking: For each input sequence, mask a certain percentage of the question tokens (e.g., 15%). The goal is to make the model learn to reconstruct the original question using the context provided by the chunk.\n",
    "\n",
    "5. Fine-tuning: Train the BERT model on your training dataset using a suitable optimizer, learning rate scheduler, and loss function. For the MLM task, you can use the cross-entropy loss between the predicted tokens and the original tokens. Monitor the model's performance on a validation set to avoid overfitting and select the best model checkpoint based on the validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First I get all the ids of the rephrase questions\n",
    "ids = [int(file.split('\\\\')[-1].split('.')[0]) for file in glob.glob('../data/questions_rephrased/*.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9881"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random_subset = pd.read_pickle('../data/raw/random_subset.pkl')\n",
    "Dokument = pd.read_pickle('../data/raw/Dokument.pkl')\n",
    "Sag = pd.read_pickle('../data/raw/Sag.pkl')\n",
    "Fil = pd.read_pickle('../data/raw/Fil.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subset_dokument = Dokument[Dokument['id'].isin(ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12049"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying the files that are linked to the rephrased questions\n",
    "Subset_fil = Fil[Fil['dokumentid'].isin(Subset_dokument['id'])]\n",
    "len(Subset_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dict that maps the fil ids to their corresponding rephrased question ids\n",
    "fil_to_question = {}\n",
    "for index, row in Subset_fil.iterrows():\n",
    "    fil_to_question[row['id']] = row['dokumentid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_to_fil = {}\n",
    "#flipping keys and values in the dict\n",
    "for key, value in fil_to_question.items():\n",
    "    question_to_fil[value] = key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152637"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_to_fil[126]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/processed/1000012.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     basename\u001b[39m.\u001b[39mappend(t)\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m basename:\n\u001b[1;32m---> 13\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m../data/processed/\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(ids[i]) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     14\u001b[0m         files\u001b[39m.\u001b[39mappend(f\u001b[39m.\u001b[39mread())  \n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/processed/1000012.txt'"
     ]
    }
   ],
   "source": [
    "#Now I can load the txt files that contain the files related to the rephrased questions\n",
    "\n",
    "#loading the txt files for the first 10 files related to the rephrased questions\n",
    "\n",
    "files = []\n",
    "basename = []\n",
    "for i in range(10):\n",
    "    txt = [question_to_fil[ids[i]]]\n",
    "    for t in txt:\n",
    "        basename.append(t)\n",
    "\n",
    "    for name in basename:\n",
    "        try:\n",
    "            with open('../data/processed/' + str(ids[i]) + '.txt', 'r') as f:\n",
    "                files.append(f.read())  \n",
    "        except:\n",
    "            print(f'File {name} not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdanlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m load_bert_base_model\n\u001b[1;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m load_bert_base_model()\n\u001b[0;32m      3\u001b[0m vecs_embedding, sentence_embedding, tokenized_text \u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39membed_text(\u001b[39m'\u001b[39m\u001b[39mHan sÃ¦lger frugt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\deep\\lib\\site-packages\\danlp\\models\\bert_models.py:690\u001b[0m, in \u001b[0;36mload_bert_base_model\u001b[1;34m(cache_dir, verbose)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_bert_base_model\u001b[39m(cache_dir\u001b[39m=\u001b[39mDEFAULT_CACHE_DIR, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    681\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[39m    Load BERT language model and use for embedding of tokens or sentence.\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[39m    The Model is trained by BotXO: https://github.com/botxo/nordic_bert\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39m    :return: BERT model \u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 690\u001b[0m     \u001b[39mreturn\u001b[39;00m BertBase(cache_dir, verbose)\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\deep\\lib\\site-packages\\danlp\\models\\bert_models.py:369\u001b[0m, in \u001b[0;36mBertBase.__init__\u001b[1;34m(self, cache_dir, verbose)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, cache_dir\u001b[39m=\u001b[39mDEFAULT_CACHE_DIR, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 369\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer, BertModel\n\u001b[0;32m    370\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m    371\u001b[0m     \u001b[39m# download model\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\import_utils.py:896\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    895\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module[name])\n\u001b[1;32m--> 896\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(module, name)\n\u001b[0;32m    897\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    898\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\import_utils.py:895\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    893\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[0;32m    894\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m--> 895\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m    896\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m    897\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\import_utils.py:905\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_module\u001b[39m(\u001b[39mself\u001b[39m, module_name: \u001b[39mstr\u001b[39m):\n\u001b[0;32m    904\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 905\u001b[0m         \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m    906\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    907\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    908\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    910\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\deep\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py:53\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpytorch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     46\u001b[0m     ModelOutput,\n\u001b[0;32m     47\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     52\u001b[0m )\n\u001b[1;32m---> 53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfiguration_bert\u001b[39;00m \u001b[39mimport\u001b[39;00m BertConfig\n\u001b[0;32m     56\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mget_logger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     58\u001b[0m _CHECKPOINT_FOR_DOC \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\configuration_bert.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Mapping\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfiguration_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m PretrainedConfig\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m OnnxConfig\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[0;32m     25\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mget_logger(\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\import_utils.py:895\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    893\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[0;32m    894\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m--> 895\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m    896\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m    897\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\import_utils.py:905\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_module\u001b[39m(\u001b[39mself\u001b[39m, module_name: \u001b[39mstr\u001b[39m):\n\u001b[0;32m    904\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 905\u001b[0m         \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m    906\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    907\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    908\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    910\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\deep\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\onnx\\config.py:35\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenization_utils_base\u001b[39;00m \u001b[39mimport\u001b[39;00m PreTrainedTokenizerBase\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m is_vision_available():\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m     37\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mget_logger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     40\u001b[0m DEFAULT_ONNX_OPSET \u001b[39m=\u001b[39m \u001b[39m11\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\deep\\lib\\site-packages\\PIL\\Image.py:100\u001b[0m\n\u001b[0;32m     91\u001b[0m MAX_IMAGE_PIXELS \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m4\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[39m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[39m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[39m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[39m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[39m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _imaging \u001b[39mas\u001b[39;00m core\n\u001b[0;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m __version__ \u001b[39m!=\u001b[39m \u001b[39mgetattr\u001b[39m(core, \u001b[39m\"\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    103\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCore version: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mgetattr\u001b[39m(core,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPillow version: \u001b[39m\u001b[39m{\u001b[39;00m__version__\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from danlp.models import load_bert_base_model\n",
    "model = load_bert_base_model()\n",
    "vecs_embedding, sentence_embedding, tokenized_text =model.embed_text('Han sÃ¦lger frugt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tells me that I definitely need to do some preprocessing on the data. Raw embeddings are not going to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\rune7\\.danlp\\bert.botxo.pytorch were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Embedding questions: 100%|ââââââââââ| 10000/10000 [10:05<00:00, 16.52it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from danlp.models import load_bert_base_model\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Load the BERT model\n",
    "model = load_bert_base_model()\n",
    "\n",
    "\n",
    "# Function to embed questions using the BERT model\n",
    "def embed_questions(questions):\n",
    "    embeddings = {}\n",
    "    #take a random subset of 10000 questions\n",
    "    \n",
    "    random_keys = random.sample(list(questions.keys()), 10000)\n",
    "    random_subset = {k: questions[k] for k in random_keys}\n",
    "\n",
    "\n",
    "    for question_id, question_text in tqdm(random_subset.items(), desc=\"Embedding questions\", total=min(10000, len(random_subset))):\n",
    "        _, sentence_embedding, _ = model.embed_text(question_text)\n",
    "        embeddings[question_id] = sentence_embedding\n",
    "    return embeddings\n",
    "\n",
    "# Embed the first 10,000 filtered questions\n",
    "question_embeddings = embed_questions(question_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User embedding shape: (768,)\n",
      "Embeddings shape: (10000, 768)\n",
      "1. Question ID: 9379, Similarity: 0.6511, Text:  Hvad vil ministeren gÃ¸re for at motivere private virksomheder til at styrke deres it-sikkerhed?\n",
      "2. Question ID: 738623, Similarity: 0.6375, Text:  StÃ¸tter regeringen Socialdemokratiets forslag om at etablere flere stenrev i kystnÃ¦re omrÃ¥der til gavn for fiskebestanden?\n",
      "3. Question ID: 899990, Similarity: 0.6180, Text:  Anser ministeren statens prioritering af udsatte og sÃ¥rbare borgere med udviklingshandicap for forsvarlig? \n",
      "4. Question ID: 4392, Similarity: 0.6103, Text:  Hvad er ministerens holdning til trÃ¦er langs vejene med hensyn til arbejde med at sikre vejenes funktion?\n",
      "5. Question ID: 712722, Similarity: 0.6049, Text:  Hvad har ministeren tÃ¦nkt sig at gÃ¸re over for den hÃ¦rvÃ¦rk, der rammer lokale erhvervsdrivende pÃ¥ NÃ¸rrebro? \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def embed_user_question(question):\n",
    "    _, user_embedding, _ = model.embed_text(question)\n",
    "    return user_embedding\n",
    "\n",
    "def find_nearest_k_questions(user_embedding, question_embeddings, k=5):\n",
    "    user_embedding = user_embedding.numpy()  # Convert user_embedding to a NumPy array\n",
    "    question_ids = np.array(list(question_embeddings.keys()))\n",
    "    \n",
    "    # Convert embeddings to NumPy arrays and stack them along a new dimension\n",
    "    embeddings = np.stack([emb.numpy() for emb in question_embeddings.values()])\n",
    "    print(\"User embedding shape:\", user_embedding.shape)\n",
    "    print(\"Embeddings shape:\", embeddings.shape)\n",
    "    similarities = 1 - cdist(user_embedding.reshape(1, -1), embeddings, metric='cosine').reshape(-1)\n",
    "\n",
    "\n",
    "    # Find the nearest k question ids\n",
    "    nearest_indices = np.argpartition(similarities, -k)[-k:]\n",
    "    nearest_question_ids = question_ids[nearest_indices]\n",
    "    nearest_similarities = similarities[nearest_indices]\n",
    "\n",
    "    # Sort the nearest questions by similarity\n",
    "    sorted_indices = np.argsort(-nearest_similarities)\n",
    "    nearest_question_ids = nearest_question_ids[sorted_indices]\n",
    "    nearest_similarities = nearest_similarities[sorted_indices]\n",
    "\n",
    "    return nearest_question_ids, nearest_similarities\n",
    "\n",
    "# User question\n",
    "user_question = \"Hvad har vi gjort for at beskytte udsatte naturomrÃ¥der ved LillebÃ¦lt?\"\n",
    "\n",
    "# Embed the user question\n",
    "user_embedding = embed_user_question(user_question)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find the nearest k questions\n",
    "k = 5\n",
    "nearest_question_ids, nearest_similarities = find_nearest_k_questions(user_embedding, question_embeddings, k)\n",
    "\n",
    "# Print the nearest questions and their similarities\n",
    "for i, (question_id, similarity) in enumerate(zip(nearest_question_ids, nearest_similarities), 1):\n",
    "    print(f\"{i}. Question ID: {question_id}, Similarity: {similarity:.4f}, Text: {question_dict[question_id]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bert_base'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m#append the models folder path to the system path\u001b[39;00m\n\u001b[0;32m      3\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mC:/Users/Rune/Documents/GitHub/knowledge-graph/src/features\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbert_base\u001b[39;00m \u001b[39mimport\u001b[39;00m BertModel\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bert_base'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#append the models folder path to the system path\n",
    "sys.path.append('C:/Users/Rune/Documents/GitHub/knowledge-graph/src/features')\n",
    "from bert_base import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def embed_user_question(question):\n",
    "    _, user_embedding, _ = model.embed_text(question)\n",
    "    return user_embedding\n",
    "\n",
    "def find_nearest_k_questions(user_embedding, question_embeddings, k=5):\n",
    "    user_embedding = user_embedding.numpy()  # Convert user_embedding to a NumPy array\n",
    "    question_ids = np.array(list(question_embeddings.keys()))\n",
    "    \n",
    "    # Convert embeddings to NumPy arrays and stack them along a new dimension\n",
    "    embeddings = np.stack([emb.numpy() for emb in question_embeddings.values()])\n",
    "    print(\"User embedding shape:\", user_embedding.shape)\n",
    "    print(\"Embeddings shape:\", embeddings.shape)\n",
    "    similarities = 1 - cdist(user_embedding.reshape(1, -1), embeddings, metric='cosine').reshape(-1)\n",
    "\n",
    "\n",
    "    # Find the nearest k question ids\n",
    "    nearest_indices = np.argpartition(similarities, -k)[-k:]\n",
    "    nearest_question_ids = question_ids[nearest_indices]\n",
    "    nearest_similarities = similarities[nearest_indices]\n",
    "\n",
    "    # Sort the nearest questions by similarity\n",
    "    sorted_indices = np.argsort(-nearest_similarities)\n",
    "    nearest_question_ids = nearest_question_ids[sorted_indices]\n",
    "    nearest_similarities = nearest_similarities[sorted_indices]\n",
    "\n",
    "    return nearest_question_ids, nearest_similarities\n",
    "\n",
    "# User question\n",
    "user_question = \"Hvad har vi gjort for at beskytte udsatte naturomrÃ¥der ved LillebÃ¦lt?\"\n",
    "\n",
    "# Embed the user question\n",
    "user_embedding = embed_user_question(user_question)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find the nearest k questions\n",
    "k = 5\n",
    "nearest_question_ids, nearest_similarities = find_nearest_k_questions(user_embedding, question_embeddings, k)\n",
    "\n",
    "# Print the nearest questions and their similarities\n",
    "for i, (question_id, similarity) in enumerate(zip(nearest_question_ids, nearest_similarities), 1):\n",
    "    print(f\"{i}. Question ID: {question_id}, Similarity: {similarity:.4f}, Text: {question_dict[question_id]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
